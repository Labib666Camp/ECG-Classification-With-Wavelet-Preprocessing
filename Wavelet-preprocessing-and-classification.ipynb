from keras import Sequential, Input, Model
from keras.layers import Conv1D, Dense, Activation, BatchNormalization, Dropout, Flatten
import pandas as pd
import matplotlib.pyplot as plt
from collections import Counter
from imblearn.over_sampling import SMOTE
import numpy as np
import pywt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder

#importing data from gdrivegithub
df1 = pd.read_csv('/content/drive/My Drive/mitbih_train.csv')
df2 = pd.read_csv('/content/drive/My Drive/mitbih_test.csv')

#data inspection
new_cols = []
_, y = df1.shape
for i in range(y):
  new_cols.append(i)
df1.columns = new_cols
df2.columns = new_cols
df2.columns = df2.columns.astype('str')
df1.columns = df1.columns.astype('str')
data = pd.concat([df1, df2], axis=0)

#quantifying the classes
equilibre = data['187'].value_counts()
print(equilibre)
data.shape

#balancing the dataset
counter = Counter(data['187'])
print(counter)
data_y = data['187']
data_x = data.drop(columns='187')
oversample = SMOTE()
data_x, data_y = oversample.fit_resample(data_x,data_y)
print(Counter(data_y))

#wavelet preprocessing
data_x = pd.DataFrame(data_x)
len = data_x.shape[0]
ca_data_x = []
cd_data_x = []
for i in range(len):
  cA, cD = pywt.dwt(data_x.iloc[i], 'haar')
  cA = np.array(cA)
  cD = np.array(cD)
  cd_data_x.append(cD)
  ca_data_x.append(cA)
cd_data_x = np.array(cd_data_x)
ca_data_x = np.array(ca_data_x)
cd_data_x = pd.DataFrame(cd_data_x)
ca_data_x = pd.DataFrame(ca_data_x)

#spliting the training and validation data
cd_train_x, cd_valid_x, train_y, valid_y = train_test_split(cd_data_x, data_y, test_size=0.33, shuffle=True)
cd_train_x = np.expand_dims(cd_train_x,2)
cd_valid_x = np.expand_dims(cd_valid_x,2)
from sklearn.preprocessing import OneHotEncoder
ohe = OneHotEncoder()
train_y = ohe.fit_transform(train_y.reshape(-1,1))
valid_y = ohe.transform(valid_y.reshape(-1,1))
train_y = train_y.toarray()
valid_y = valid_y.toarray()

#creating the model
input_shape = (94,1)
input_data = Input(shape=input_shape, name='input_data')
print(input_data.shape)
cn1 = Conv1D(64,(5),strides=1, activation="relu", input_shape=input_shape)(input_data)
print(cn1.shape)
cn1 = BatchNormalization()(cn1)

cn2 = Conv1D(64,(5),strides=1, activation="relu")(cn1)
print(cn2.shape)
cn2 = BatchNormalization()(cn2)

cn3 = Conv1D(64,(5),strides=1, activation="relu")(cn2)
print(cn3.shape)
cn3 = BatchNormalization()(cn3)

flat = Flatten()(cn3)
print(flat.shape)
d1 = Dense(64, activation="relu")(flat)
print(d1.shape)
d2 = Dense(32, activation="relu")(d1)
print(d2.shape)
output_d3 = Dense(5, activation="softmax", name='d3')(d2)
print(output_d3.shape)
model = Model(inputs=input_data, outputs=output_d3)
assert model.output_shape == (None,5)
model.compile(optimizer='adam', loss='categorical_crossentropy',metrics = ['accuracy'])
history = model.fit(cd_train_x, train_y, epochs=10, batch_size=32, validation_data=(cd_valid_x,valid_y))

#plotting accuracy and validation
fig1, ax_acc = plt.subplots()
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.title('Model - Accuracy')
plt.legend(['Training', 'Validation'], loc='lower right')
plt.show()
fig2, ax_loss = plt.subplots()
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Model- Loss')
plt.legend(['Training', 'Validation'], loc='upper right')
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.show()
